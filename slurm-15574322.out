INFO[2023-10-19 14:40:47,111]: Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
INFO[2023-10-19 14:40:47,111]: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
INFO[2023-10-19 14:40:47,113]: Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
INFO[2023-10-19 14:40:47,113]: Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
WARNING[2023-10-19 14:40:47,113]: No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
/data/uqhmcdou/paralleltest/example_slurmjob.py:55: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.
  sampler = npy.infer.MCMC(npy.infer.NUTS(model=model),
-------------------------------------------------------------------------------
Doing Imports
-------------------------------------------------------------------------------
Doing NumPyro setup
Starting program with 1 cores registering from jax.local_device_count()
-------------------------------------------------------------------------------
Doing MCMC
  0%|          | 0/800 [00:00<?, ?it/s]warmup:   0%|          | 1/800 [00:02<32:10,  2.42s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  29%|██▉       | 231/800 [00:02<00:04, 128.80it/s, 1 steps of size 2.59e-03. acc. prob=0.77]sample:  59%|█████▊    | 469/800 [00:02<00:01, 295.08it/s, 27 steps of size 6.04e-03. acc. prob=0.93]sample:  88%|████████▊ | 708/800 [00:02<00:00, 494.84it/s, 9 steps of size 6.04e-03. acc. prob=0.93] sample: 100%|██████████| 800/800 [00:02<00:00, 290.29it/s, 43 steps of size 6.04e-03. acc. prob=0.93]
  0%|          | 0/800 [00:00<?, ?it/s]warmup:  30%|██▉       | 238/800 [00:00<00:00, 2371.91it/s, 11 steps of size 6.79e-03. acc. prob=0.77]sample:  60%|█████▉    | 476/800 [00:00<00:00, 2336.46it/s, 151 steps of size 9.02e-03. acc. prob=0.93]sample:  89%|████████▉ | 710/800 [00:00<00:00, 2321.27it/s, 3 steps of size 9.02e-03. acc. prob=0.94]  sample: 100%|██████████| 800/800 [00:00<00:00, 2326.68it/s, 27 steps of size 9.02e-03. acc. prob=0.94]
-------------------------------------------------------------------------------
Doing Nested Sampling
Done
-------------------------------------------------------------------------------

                mean       std    median      5.0%     95.0%     n_eff     r_hat
         c      2.00      0.05      2.00      1.91      2.07    158.31      1.02
         m      1.00      0.00      1.00      1.00      1.00    159.04      1.02

Number of divergences: 0
--------
Termination Conditions:
Reached max samples
Small remaining evidence
--------
# likelihood evals: 7469877
# samples: 50000
# slices: 971268.0
# slices / acceptance: 29.0
# likelihood evals / sample: 149.4
# likelihood evals / slice: 7.6
--------
logZ=-28.48 +- 0.11
H=28.0
ESS=49
--------
c: mean +- std.dev. | 10%ile / 50%ile / 90%ile | MAP est. | max(L) est.
c: 2.017 +- 0.044 | 1.953 / 2.025 / 2.064 | 1.986 | 1.986
--------
m: mean +- std.dev. | 10%ile / 50%ile / 90%ile | MAP est. | max(L) est.
m: 0.9994 +- 0.0015 | 0.9976 / 0.999 / 1.0017 | 1.0005 | 1.0005
--------
-------------------------------------------------------------------------------
