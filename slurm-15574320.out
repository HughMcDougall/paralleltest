INFO[2023-10-19 14:40:41,915]: Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
INFO[2023-10-19 14:40:41,915]: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
INFO[2023-10-19 14:40:41,917]: Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
INFO[2023-10-19 14:40:41,917]: Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
WARNING[2023-10-19 14:40:41,917]: No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
/data/uqhmcdou/paralleltest/example_slurmjob.py:55: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.
  sampler = npy.infer.MCMC(npy.infer.NUTS(model=model),
-------------------------------------------------------------------------------
Doing Imports
-------------------------------------------------------------------------------
Doing NumPyro setup
Starting program with 1 cores registering from jax.local_device_count()
-------------------------------------------------------------------------------
Doing MCMC
  0%|          | 0/800 [00:00<?, ?it/s]warmup:   0%|          | 1/800 [00:02<33:08,  2.49s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  22%|██▏       | 175/800 [00:02<00:06, 94.82it/s, 7 steps of size 4.42e-03. acc. prob=0.76]sample:  54%|█████▎    | 428/800 [00:02<00:01, 270.00it/s, 55 steps of size 6.04e-03. acc. prob=0.93]sample:  85%|████████▌ | 681/800 [00:02<00:00, 480.59it/s, 39 steps of size 6.04e-03. acc. prob=0.93]sample: 100%|██████████| 800/800 [00:02<00:00, 282.06it/s, 43 steps of size 6.04e-03. acc. prob=0.93]
  0%|          | 0/800 [00:00<?, ?it/s]warmup:  31%|███       | 248/800 [00:00<00:00, 2476.05it/s, 1 steps of size 2.97e-03. acc. prob=0.77]sample:  62%|██████▏   | 496/800 [00:00<00:00, 2398.29it/s, 3 steps of size 9.02e-03. acc. prob=0.94]sample:  92%|█████████▏| 737/800 [00:00<00:00, 2398.80it/s, 143 steps of size 9.02e-03. acc. prob=0.94]sample: 100%|██████████| 800/800 [00:00<00:00, 2405.08it/s, 27 steps of size 9.02e-03. acc. prob=0.94] 
-------------------------------------------------------------------------------
Doing Nested Sampling
Done
-------------------------------------------------------------------------------

                mean       std    median      5.0%     95.0%     n_eff     r_hat
         c      2.00      0.05      2.00      1.91      2.07    158.31      1.02
         m      1.00      0.00      1.00      1.00      1.00    159.04      1.02

Number of divergences: 0
--------
Termination Conditions:
Reached max samples
Small remaining evidence
--------
# likelihood evals: 7469877
# samples: 50000
# slices: 971268.0
# slices / acceptance: 29.0
# likelihood evals / sample: 149.4
# likelihood evals / slice: 7.6
--------
logZ=-28.48 +- 0.11
H=28.0
ESS=49
--------
c: mean +- std.dev. | 10%ile / 50%ile / 90%ile | MAP est. | max(L) est.
c: 2.017 +- 0.044 | 1.953 / 2.025 / 2.064 | 1.986 | 1.986
--------
m: mean +- std.dev. | 10%ile / 50%ile / 90%ile | MAP est. | max(L) est.
m: 0.9994 +- 0.0015 | 0.9976 / 0.999 / 1.0017 | 1.0005 | 1.0005
--------
-------------------------------------------------------------------------------
